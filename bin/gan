#!/usr/bin/env python3

import os
libpath = os.path.normpath(                                                    \
            os.path.join(                                                      \
                os.path.dirname(os.path.abspath(os.path.realpath(__file__))),  \
                '..')                                                          \
            )
import sys
sys.path.append(libpath)

import distutils.util
import argparse
import csv
import numpy

import matplotlib
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter

import tensorflow       as     tf
from   tensorflow.data  import Dataset
from   tensorflow.keras import layers, Model, Sequential, optimizers, losses, \
                               metrics, callbacks, initializers
from tensorflow.keras.layers.experimental import preprocessing

import tensorflow_datasets as tfds

from   gmcdp._version     import __version__
from   gmcdp.GausNoiseOn  import GausNoiseOn
from   gmcdp.spectral_normalization import SpectralNormalization

### class definitions ----------------------------------------------------------

# generative adversarial network
class GAN(Model):

  def __init__(self, disc, gen, latent_dim):
    super(GAN, self).__init__()
    self.discriminator = disc
    self.generator     = gen
    self.latent_dim    = latent_dim
    return

  def compile(self, optD, optG, loss_fn):
    super(GAN, self).compile()
    self.optD    = optD
    self.optG    = optG
    self.loss_fn = loss_fn
    # gradient penalty weight
    #self.gp_weight = 10.0
    return

  #def grad_penalty(self, data):
  #  with tf.GradientTape() as gp_tape:
  #    gp_tape.watch(data)
  #    pred = self.discriminator(data, training=True)
  #  gp_grds = gp_tape.gradient(pred, [data])[0]
  #  norm    = tf.sqrt(tf.reduce_sum(tf.square(gp_grds), axis=[1,2]))
  #  gp      = tf.reduce_mean((norm-1.0)**2)
  #  return gp

  def train_step(self, real_data):
    if isinstance(real_data, tuple):
      real_data = real_data[0]
    # batch size
    bs = tf.shape(real_data)[0]

    # train discriminator on real data
    with tf.GradientTape() as tape:
      labels  = -tf.ones((bs,1))
      preds   = self.discriminator(real_data, training=True)
      #cost    = self.loss_fn(labels, preds)
      #gp      = self.grad_penalty(real_data)
      #d_lossR = cost + gp * self.gp_weight
      d_lossR = self.loss_fn(labels, preds)
      grads   = tape.gradient(d_lossR, self.discriminator.trainable_weights)
      self.optD.apply_gradients(zip(grads,
                                    self.discriminator.trainable_weights))

    # train discriminator on fake data
    with tf.GradientTape() as tape:
      # generate fake data
      z = tf.random.normal(shape=(bs, self.latent_dim))
      fake_data = self.generator(z, training=True)
      # train
      labels  = tf.ones((bs,1))
      preds   = self.discriminator(fake_data, training=True)
      #cost    = self.loss_fn(labels, preds)
      #gp      = self.grad_penalty(fake_data)
      #d_lossG = cost + gp * self.gp_weight
      d_lossG = self.loss_fn(labels, preds)
      grads   = tape.gradient(d_lossG, self.discriminator.trainable_weights)
      self.optD.apply_gradients(zip(grads,
                                    self.discriminator.trainable_weights))

    # train generator once
    with tf.GradientTape() as tape:
      # create misleading labels for generator
      misleading_labels = -tf.ones((bs,1))
      z = tf.random.normal(shape=(bs, self.latent_dim))
      # train
      fake_preds = self.discriminator(self.generator(z, training=True))
      g_loss     = self.loss_fn(misleading_labels, fake_preds)
      grads      = tape.gradient(g_loss, self.generator.trainable_weights)
      self.optG.apply_gradients(zip(grads, self.generator.trainable_weights))

    return {'d_loss_real':d_lossR, 'd_loss_fake':d_lossG, 'g_loss':g_loss}


  def summary(self, line_length=None, positions=None, print_fn=None):
    self.discriminator.summary(line_length, positions, print_fn)
    self.generator.summary(line_length, positions, print_fn)
    return

### function definitions -------------------------------------------------------

### build discriminator model
def build_discriminator(data_dim):
  fltrs      = 256
  filtersize = 5
  init       = initializers.RandomNormal(mean=0.0, stddev=0.02)

  model = Sequential(name='discriminator')
  model.add(SpectralNormalization(layers.Conv1D(fltrs//2, filtersize,
                                  padding='same',
                                  kernel_initializer=init),
                                  input_shape=(data_dim,1,)))
  model.add(layers.LeakyReLU())

  model.add(SpectralNormalization(layers.Conv1D(fltrs, filtersize,
                                  padding='same',
                                  kernel_initializer=init)))
  model.add(layers.LeakyReLU())
  model.add(layers.LayerNormalization())

  model.add(SpectralNormalization(layers.Conv1D(fltrs//2, filtersize,
                                  padding='same',
                                  kernel_initializer=init)))
  model.add(layers.LeakyReLU())
  model.add(layers.LayerNormalization())

  model.add(layers.Flatten())
  model.add(SpectralNormalization(layers.Dense(fltrs//2,
                                  kernel_initializer=init)))
  model.add(layers.LeakyReLU())
  model.add(SpectralNormalization(layers.Dense(1, kernel_initializer=init)))
  return model


### build a generator block
def gen_block(inlayer, n_neurons, block_id, filtersize, use_bias, init):
  conv = layers.Conv1DTranspose(n_neurons, filtersize,
                                padding='same',
                                use_bias=use_bias,
                                kernel_initializer=init,
                                name='convt_'+str(block_id))(inlayer)
  bn = layers.BatchNormalization(name='bn_'+str(block_id))(conv)
  mu = layers.LeakyReLU(name='mu_'+str(block_id))(bn)
  mp = layers.MaxPool1D(filtersize, 1,
                        padding='same',
                        name='pool_'+str(block_id))(mu)
  gn = GausNoiseOn(stddev=1.0, name='noise_'+str(block_id))(mp)
  Lout = layers.Add(name='out_'+str(block_id))([mu,gn])
  return Lout

### build generator model
def build_generator(data_dim, blocks, nrons):
  filtersize = 5
  use_bias   = False
  init       = initializers.RandomNormal(mean=0.0, stddev=0.02)

  #Lin  = layers.Input(shape=(latent_dim), name='gen_in')
  #Ld   = layers.Dense(data_dim*nrons, use_bias=use_bias,
  #                                    kernel_initializer=init,
  #                                    name='dense_in')(Lin)
  #bn   = layers.BatchNormalization(name='bn_in')(Ld)
  #af   = layers.LeakyReLU(name='relu_in')(bn)
  #tout = layers.Reshape((data_dim, nrons), name='reshape_in')(af)

  Lin  = layers.Input(shape=(data_dim), name='gen_in')
  rshp = layers.Reshape((data_dim, 1), name='reshape_in')(Lin)
  lstm = layers.Bidirectional(layers.LSTM(nrons, kernel_initializer=init,
                              return_sequences=True), name='lstm')(rshp)
  tout = layers.BatchNormalization(name='bn_in')(lstm)

  for i in range(blocks):
    if i == 0:
      tout = gen_block(tout, nrons, i, filtersize, use_bias, init)
    else:
      tout = gen_block(tout, nrons//2, i, filtersize, use_bias, init)

  Lout = layers.Conv1DTranspose(1, filtersize,
                                padding='same',
                                use_bias=use_bias,
                                kernel_initializer=init,
                                name='gen_out')(tout)

  return Model(inputs=Lin, outputs=Lout, name='generator')


### store samples from generator in points_list
def store_samples(epoch, logs, generator, z, points_list, log_lists):
  every = 20
  if epoch % every == 0:
    generated_data = generator(z).numpy()
    points_list.append(generated_data)
    for k in log_lists.keys():
      if k in logs.keys():
        log_lists[k].append(logs[k])
    return

### build Dataset from data file
def get_real_data(filename, buffersize, batchsize):
  indata = []
  with open(filename, 'r') as handle:
    reader = csv.reader(handle)
    for row in reader:
      indata.append([[float(x)] for x in row])
  data = Dataset.from_tensor_slices(indata)
  data = data.shuffle(buffersize).batch(batchsize)
  return data

### wasserstein loss function
# implements 'earth mover' distance from https://arxiv.org/pdf/1701.07875.pdf
# and https://arxiv.org/pdf/1704.00028.pdf
@tf.function
def wasserstein_loss(tlabs, plabs):
  return tf.reduce_mean(tlabs * plabs)

### main -----------------------------------------------------------------------
if __name__ == '__main__':
  parser = argparse.ArgumentParser(
                description='calculates profiles from a pssm txt file',
                formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  parser.add_argument('--version', action='version', version=__version__)

  # data source
  parser.add_argument('-f', '--file', dest='file', help='data file',
                      metavar='DATA.csv', required=True)
  parser.add_argument('--data_dim', dest='data_dim', type=int,
                      help='size of data vector', metavar='N')

  # data pre-processing
  parser.add_argument('--buffer_size', dest='buffer_size', type=int,
                      help='data shuffling buffer', metavar='N')
  parser.add_argument('--batch_size', dest='batch_size', type=int,
                      help='training batch size', metavar='N')

  # neural network architecture
  parser.add_argument('--gen_blocks', dest='gen_blocks', type=int,
                      help='number of generator blocks', metavar='N')
  parser.add_argument('--filters', dest='n_filters', type=int,
                      help='number of neurons or filters', metavar='N')

  # training regime
  parser.add_argument('--epochs', dest='epochs', type=int,
                      help='number of training epochs', metavar='N')

  parser.set_defaults(file=None,
                      data_dim=256,
                      buffer_size=10000,
                      batch_size=64,
                      gen_blocks=3,
                      n_filters=128,
                      epochs=1000)

  args = parser.parse_args()

  # get 'real' data
  train_data = get_real_data(args.file,
                             args.buffer_size,
                             args.batch_size)
  print(train_data.element_spec)

  # build discriminator and generator models
  # implements spectral normalization of discriminator weights
  # from https://arxiv.org/pdf/1802.05957.pdf
  discriminator = build_discriminator(args.data_dim)
  generator     = build_generator(args.data_dim,
                                  args.gen_blocks,
                                  args.n_filters)

  # build GAN
  gan = GAN(discriminator, generator, args.data_dim)
  # implements 2-timescale update rule from https://arxiv.org/pdf/1706.08500.pdf
  disc_lr = 3.0e-4
  genr_lr = 1.0e-4
  gan.compile(optimizers.Adam(learning_rate=disc_lr, beta_1=0.0, beta_2=0.9),
              optimizers.Adam(learning_rate=genr_lr, beta_1=0.0, beta_2=0.9),
              wasserstein_loss)

  gan.summary()

  # set up generator logging
  n_samples = 3
  real_points_list = [ x.ravel() for x in tfds.as_numpy(train_data.take(1)\
                                                        .unbatch()\
                                                        .take(n_samples)) ]
  sample_zs = tf.random.normal(shape=(n_samples, args.data_dim))

  gen_points_list = []
  log_lists = {'d_loss_real':[], 'd_loss_fake':[], 'g_loss':[]}
  cbk = callbacks.LambdaCallback(on_epoch_end=lambda epoch,logs: \
                                              store_samples(epoch, logs,
                                                            gan.generator,
                                                            sample_zs,
                                                            gen_points_list,
                                                            log_lists))

  # fit generative adverserial network
  gan.fit(train_data, epochs=args.epochs, callbacks=[cbk], verbose=True)

  # save generator model
  genfname = args.file + '.gen.model'
  gan.generator.save(genfname)


  ### plot results
  matplotlib.rcParams.update({'font.size':8})

  x = numpy.linspace(1, args.data_dim, num=args.data_dim)
  gens = numpy.arange(0, len(gen_points_list))

  fig,axs = plt.subplots(nrows=3, ncols=n_samples, figsize=(n_samples,3),
                         sharex=True, sharey=True)

  # set up example data logging
  axs[0,0].set_ylabel('real data')
  axs[1,0].set_ylabel('fake data')
  dyn_data = []
  for i in range(n_samples):
    axs[0,i].get_xaxis().set_ticks([])
    axs[0,i].get_yaxis().set_ticks([])
    axs[1,i].get_xaxis().set_ticks([])
    axs[1,i].get_yaxis().set_ticks([])
    ax = axs[0,i]
    y  = real_points_list[i]
    ax.plot(x,y, 'bo', markersize=1)
    dd, = axs[1,i].plot([],[], 'ro', markersize=1)
    dyn_data.append(dd)

  # set up loss logging
  lsax = plt.subplot2grid((3,n_samples), (2,0), colspan=n_samples)
  lsax.set_xlim([0, len(gen_points_list)-1])
  min_loss = numpy.amin([x for x in log_lists.values()])
  max_loss = numpy.amax([x for x in log_lists.values()])
  lsax.set_ylim([min_loss, max_loss])
  lsax.set_ylabel('loss')
  lsax.set_xlabel('generation')
  lsax.get_xaxis().set_ticks([])
  lsax.get_yaxis().set_ticks([])
  for loss in log_lists.keys():
    dd, = lsax.plot([],[], linewidth=0.5, label=loss)
    dyn_data.append(dd)
  lsax.legend(loc='upper left', borderpad=0.2,
                                labelspacing=0.2,
                                handlelength=0.5,
                                handletextpad=0.2)

  plt.tight_layout()

  def update(data_idx):
    # example data
    data = gen_points_list[data_idx]
    for i in range(n_samples):
      y  = data[i,:,:].ravel()
      dyn_data[i].set_data(x,y)
    # losses
    loss_x = gens[:data_idx+1]
    i = n_samples
    for loss in log_lists.values():
      loss_y = loss[:data_idx+1]
      dyn_data[i].set_data(loss_x, loss_y)
      i += 1
    return dyn_data,

  anim = FuncAnimation(fig, update,
                       frames=numpy.arange(0,len(gen_points_list)),
                       interval=200)
  #plt.show()

  writer = PillowWriter(fps=4)
  gifname = args.file + '.out.gif'
  anim.save(gifname, writer=writer)
