#!/usr/bin/env python3

import os
libpath = os.path.normpath(                                                    \
            os.path.join(                                                      \
                os.path.dirname(os.path.abspath(os.path.realpath(__file__))),  \
                '..')                                                          \
            )
import sys
sys.path.append(libpath)

import distutils.util
import argparse
import csv
import numpy

import tensorflow       as     tf
from   tensorflow.data  import Dataset
from   tensorflow.keras import layers, Model, Sequential, optimizers, losses, \
                               metrics, callbacks, initializers
from tensorflow.keras.layers.experimental import preprocessing

from   gmcdp._version     import __version__
from   gmcdp.GausNoiseOn  import GausNoiseOn
from   gmcdp.spectral_normalization import SpectralNormalization

### class definitions ----------------------------------------------------------

# generative adversarial network
class GAN(Model):

  def __init__(self, disc, gen, latent_dim, gradpenalty):
    super(GAN, self).__init__()
    self.discriminator = disc
    self.generator     = gen
    self.latent_dim    = latent_dim
    self.gradpenalty   = gradpenalty
    return

  def compile(self, optD, optG, loss_fn):
    super(GAN, self).compile()
    self.optD    = optD
    self.optG    = optG
    self.loss_fn = loss_fn
    # gradient penalty weight
    self.gp_weight = 10.0
    return

  def grad_penalty(self, data):
    with tf.GradientTape() as gp_tape:
      gp_tape.watch(data)
      pred = self.discriminator(data, training=True)
    gp_grds = gp_tape.gradient(pred, [data])[0]
    norm    = tf.sqrt(tf.reduce_sum(tf.square(gp_grds), axis=[1,2]))
    gp      = tf.reduce_mean((norm-1.0)**2)
    return gp

  def train_step(self, real_data):
    if isinstance(real_data, tuple):
      real_data = real_data[0]
    # batch size
    bs = tf.shape(real_data)[0]

    # train discriminator on real data
    with tf.GradientTape() as tape:
      labels  = -tf.ones((bs,1))
      preds   = self.discriminator(real_data, training=True)
      d_lossR = self.loss_fn(labels, preds)
      if self.gradpenalty:
        gp      = self.grad_penalty(real_data)
        d_lossR += gp * self.gp_weight
      grads   = tape.gradient(d_lossR, self.discriminator.trainable_weights)
      self.optD.apply_gradients(zip(grads,
                                    self.discriminator.trainable_weights))

    # train discriminator on fake data
    with tf.GradientTape() as tape:
      # generate fake data
      z = tf.random.normal(shape=(bs, self.latent_dim))
      fake_data = self.generator(z, training=True)
      # train
      labels  = tf.ones((bs,1))
      preds   = self.discriminator(fake_data, training=True)
      d_lossG = self.loss_fn(labels, preds)
      if self.gradpenalty:
        gp      = self.grad_penalty(fake_data)
        d_lossG += gp * self.gp_weight
      grads   = tape.gradient(d_lossG, self.discriminator.trainable_weights)
      self.optD.apply_gradients(zip(grads,
                                    self.discriminator.trainable_weights))

    # train generator once
    with tf.GradientTape() as tape:
      # create misleading labels for generator
      misleading_labels = -tf.ones((bs,1))
      z = tf.random.normal(shape=(bs, self.latent_dim))
      # train
      fake_preds = self.discriminator(self.generator(z, training=True))
      g_loss     = self.loss_fn(misleading_labels, fake_preds)
      grads      = tape.gradient(g_loss, self.generator.trainable_weights)
      self.optG.apply_gradients(zip(grads, self.generator.trainable_weights))

    return {'d_loss_real':d_lossR, 'd_loss_fake':d_lossG, 'g_loss':g_loss}


  def summary(self, line_length=None, positions=None, print_fn=None):
    self.discriminator.summary(line_length, positions, print_fn)
    self.generator.summary(line_length, positions, print_fn)
    return

### function definitions -------------------------------------------------------

## builds discriminator using gradient penalty
def _disc_gp(data_dim, blocks, fltrs):
  filtersize = 5
  init       = initializers.RandomNormal(mean=0.0, stddev=0.02)

  model = Sequential(name='discriminator')
  model.add(layers.Conv1D(fltrs, filtersize,
                          padding='same',
                          kernel_initializer=init,
                          input_shape=(data_dim,1,)))
  model.add(layers.LeakyReLU())

  for b in range(blocks):
    if b == blocks-1:
      model.add(layers.Conv1D(fltrs//2, filtersize,
                              padding='same',
                              kernel_initializer=init))
    else:
      model.add(layers.Conv1D(fltrs, filtersize,
                              padding='same',
                              kernel_initializer=init))
    model.add(layers.LeakyReLU())
    model.add(layers.LayerNormalization())

  model.add(layers.Flatten())
  model.add(layers.Dense(fltrs//2, kernel_initializer=init))
  model.add(layers.Dense(1, kernel_initializer=init))
  return model

## builds discriminator using spectral normalization
def _disc_sn(data_dim, blocks, fltrs):
  filtersize = 5
  init       = initializers.RandomNormal(mean=0.0, stddev=0.02)

  model = Sequential(name='discriminator')
  model.add(SpectralNormalization(layers.Conv1D(fltrs//2, filtersize,
                                  padding='same',
                                  kernel_initializer=init),
                                  input_shape=(data_dim,1,)))
  model.add(layers.LeakyReLU())

  for b in range(blocks):
    if b == blocks-1:
      model.add(SpectralNormalization(layers.Conv1D(fltrs//2, filtersize,
                                      padding='same',
                                      kernel_initializer=init)))
    else:
      model.add(SpectralNormalization(layers.Conv1D(fltrs, filtersize,
                                      padding='same',
                                      kernel_initializer=init)))
    model.add(layers.LeakyReLU())
    model.add(layers.LayerNormalization())

  model.add(layers.Flatten())
  model.add(SpectralNormalization(layers.Dense(fltrs//2,
                                  kernel_initializer=init)))
  model.add(layers.LeakyReLU())
  model.add(SpectralNormalization(layers.Dense(1, kernel_initializer=init)))
  return model

### build discriminator model
def build_discriminator(data_dim, blocks, nrons, grad_penalty):
  if grad_penalty:
    return _disc_gp(data_dim, blocks, nrons)
  return _disc_sn(data_dim, blocks, nrons)


### build a generator block
# implements 'noise injection' into layers from
#   https://arxiv.org/pdf/1812.04948.pdf
#   and https://arxiv.org/pdf/2006.05891.pdf
def gen_block(inlayer, n_neurons, block_id, filtersize, use_bias, init):
  conv = layers.Conv1DTranspose(n_neurons, filtersize,
                                padding='same',
                                use_bias=use_bias,
                                kernel_initializer=init,
                                name='convt_'+str(block_id))(inlayer)
  bn = layers.BatchNormalization(name='bn_'+str(block_id))(conv)
  mu = layers.LeakyReLU(name='mu_'+str(block_id))(bn)
  mp = layers.MaxPool1D(filtersize, 1,
                        padding='same',
                        name='pool_'+str(block_id))(mu)
  gn = GausNoiseOn(stddev=1.0, name='noise_'+str(block_id))(mp)
  Lout = layers.Add(name='out_'+str(block_id))([mu,gn])
  return Lout

### build generator model
def build_generator(data_dim, blocks, nrons):
  filtersize = 5
  use_bias   = False
  init       = initializers.RandomNormal(mean=0.0, stddev=0.02)

  #Lin  = layers.Input(shape=(latent_dim), name='gen_in')
  #Ld   = layers.Dense(data_dim*nrons, use_bias=use_bias,
  #                                    kernel_initializer=init,
  #                                    name='dense_in')(Lin)
  #bn   = layers.BatchNormalization(name='bn_in')(Ld)
  #af   = layers.LeakyReLU(name='relu_in')(bn)
  #tout = layers.Reshape((data_dim, nrons), name='reshape_in')(af)

  Lin  = layers.Input(shape=(data_dim), name='gen_in')
  rshp = layers.Reshape((data_dim, 1), name='reshape_in')(Lin)
  lstm = layers.Bidirectional(layers.LSTM(nrons, kernel_initializer=init,
                              return_sequences=True), name='lstm')(rshp)
  tout = layers.BatchNormalization(name='bn_in')(lstm)

  for i in range(blocks):
    tout = gen_block(tout, nrons, i, filtersize, use_bias, init)

  Lout = layers.Conv1DTranspose(1, filtersize,
                                padding='same',
                                use_bias=use_bias,
                                kernel_initializer=init,
                                name='gen_out')(tout)

  return Model(inputs=Lin, outputs=Lout, name='generator')

### write log data to output file
def writelog(epoch, logs, outf):
  outf.write(str(epoch))
  for k in logs.keys():
    outf.write(',{:.4f}'.format(logs[k]))
  outf.write('\n')
  return

### saves generator model in genfname file (directory)
### writes logs
def store_model(epoch, logs, generator, genfname):
  # save logs
  lfname = genfname + '.log'
  if epoch == 0:
    with open(lfname,'w') as outf:
      outf.write('epoch,' + ','.join(list(logs.keys())) + '\n')
      writelog(epoch, logs, outf)
  else:
    with open(lfname,'a') as outf:
      writelog(epoch, logs, outf)
  # save generator model
  every = 500
  if epoch <= 1000:
    every = 100
  if epoch % every == 0:
    generator.save(genfname + str(epoch))
  return

### build Dataset from data file
def get_real_data(filename, buffersize, batchsize):
  indata = []
  with open(filename, 'r') as handle:
    reader = csv.reader(handle)
    for row in reader:
      indata.append([[float(x)] for x in row])
  data = Dataset.from_tensor_slices(indata)
  data = data.shuffle(buffersize).batch(batchsize)
  return data

### wasserstein loss function
# implements 'earth mover' distance from https://arxiv.org/pdf/1701.07875.pdf
# and https://arxiv.org/pdf/1704.00028.pdf
@tf.function
def wasserstein_loss(tlabs, plabs):
  return tf.reduce_mean(tlabs * plabs)

### main -----------------------------------------------------------------------
if __name__ == '__main__':
  parser = argparse.ArgumentParser(
                description='calculates profiles from a pssm txt file',
                formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  parser.add_argument('--version', action='version', version=__version__)
  parser.add_argument('--verbose', type=distutils.util.strtobool,
                      dest='verbose', help='runtime informatoin?',
                      metavar='y|n')

  # data source
  parser.add_argument('-f', '--file', dest='file', help='data file',
                      metavar='DATA.csv', required=True)
  parser.add_argument('--outdir', dest='outdir',
                      help='output directory', metavar='OUT')
  parser.add_argument('--data_dim', dest='data_dim', type=int,
                      help='size of data vector', metavar='N')

  # data pre-processing
  parser.add_argument('--buffer_size', dest='buffer_size', type=int,
                      help='data shuffling buffer', metavar='N')
  parser.add_argument('--batch_size', dest='batch_size', type=int,
                      help='training batch size', metavar='N')

  # neural network architecture
  parser.add_argument('--gen_blocks', dest='gen_blocks', type=int,
                      help='number of generator blocks', metavar='N')
  parser.add_argument('--filters', dest='n_filters', type=int,
                      help='number of neurons or filters', metavar='N')
  parser.add_argument('--lipschitz', dest='lipschitz', choices=['gp', 'sn'],
                      help='lipschitz constraint to use')

  # training regime
  parser.add_argument('--epochs', dest='epochs', type=int,
                      help='number of training epochs', metavar='N')
  parser.add_argument('--optimizer', dest='optimizer',
                      choices=['adam', 'rmsprop'],
                      help='optimizer to use')
  parser.add_argument('--learning_rate', dest='learn_rate', type=float,
                      help='base learning rate for optimizer', metavar='N')

  parser.set_defaults(verbose=True,
                      file=None,
                      outdir=None,
                      data_dim=256,
                      buffer_size=10000,
                      batch_size=64,
                      gen_blocks=4,
                      n_filters=128,
                      lipschitz='gp',
                      optimizer='adam',
                      learn_rate=1.0e-4,
                      epochs=1000)

  args = parser.parse_args()

  # set tensorflow log level
  if not args.verbose:
    os.environ['TF_CPP_MIN_LOG_LEVEL']='1'

  # get 'real' data
  train_data = get_real_data(args.file,
                             args.buffer_size,
                             args.batch_size)
  if args.verbose:
    print(train_data.element_spec)

  # build discriminator and generator models
  # sp implements spectral normalization of discriminator weights
  #    https://arxiv.org/pdf/1802.05957.pdf
  # gp implements gradient penalty for discriminator
  #    https://arxiv.org/pdf/1704.00028.pdf
  gradpenalty = False
  if args.lipschitz == 'gp':
    gradpenalty = True
  discriminator = build_discriminator(args.data_dim,
                                      args.gen_blocks,
                                      args.n_filters,
                                      gradpenalty)
  generator     = build_generator(args.data_dim,
                                  args.gen_blocks,
                                  args.n_filters)

  # build GAN
  gan = GAN(discriminator, generator, args.data_dim, gradpenalty)

  # set up optimizers
  # implements 2-timescale update rule from https://arxiv.org/pdf/1706.08500.pdf
  genr_lr = args.learn_rate
  disc_lr = 2.0 * genr_lr
  genr_opt = optimizers.RMSprop(learning_rate=genr_lr)
  disc_opt = optimizers.RMSprop(learning_rate=disc_lr)
  if args.optimizer == 'adam':
    genr_opt = optimizers.Adam(learning_rate=genr_lr, beta_1=0.5, beta_2=0.9)
    disc_opt = optimizers.Adam(learning_rate=disc_lr, beta_1=0.5, beta_2=0.9)
  gan.compile(disc_opt, genr_opt, wasserstein_loss)

  if args.verbose:
    gan.summary()

  # set up output base name
  outbasename = args.file
  if args.outdir:
    basename = os.path.basename(args.file)
    if not os.path.exists(args.outdir):
      sys.stderr.write('ERRR: outdir {} in existential crisis'\
                       .format(args.outdir))
      sys.exit(1)
    outbasename = os.path.join(args.outdir, basename)

  # set up checkpoints
  #ckptfname = outbasename + '.ckpt'
  #ckpt = callbacks.ModelCheckpoint(ckptfname, monitor='g_loss')

  # set up tensorboard
  #tbdir = outbasename + '.tblog'
  #tb = callbacks.TensorBoard(log_dir=tbdir)

  # set up generator logging
  genfname  = outbasename + '.gen.model'
  cbk = callbacks.LambdaCallback(on_epoch_end=lambda epoch,logs: \
                                              store_model(epoch, logs,
                                                          gan.generator,
                                                          genfname))

  # fit generative adverserial network
  gan.fit(train_data, epochs=args.epochs, callbacks=[cbk], verbose=args.verbose)

  # save final generator model
  gan.generator.save(genfname)
