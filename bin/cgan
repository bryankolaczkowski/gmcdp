#!/usr/bin/env python3

import os
libpath = os.path.normpath(                                                    \
            os.path.join(                                                      \
                os.path.dirname(os.path.abspath(os.path.realpath(__file__))),  \
                '..')                                                          \
            )
import sys
sys.path.append(libpath)

import distutils.util
import argparse
import pandas
import numpy

import tensorflow       as     tf
from   tensorflow.data  import Dataset

from gmcdp._version import __version__
from gmcdp.gen1d    import CondGen1D
from gmcdp.dis1d    import CondDis1D
from gmcdp.gan1d    import GanOptimizer, CondGan1D


### write log data to output file
def writelog(epoch, logs, outf):
  outf.write(str(epoch))
  for k in logs.keys():
    outf.write(',{:.4e}'.format(logs[k]))
  outf.write('\n')
  return

### saves generator model in genfname file (directory)
### writes logs
def store_model(epoch, logs, generator, genfname):
  # save logs
  lfname = genfname + '.log'
  if epoch == 0:
    with open(lfname,'w') as outf:
      outf.write('epoch,' + ','.join(list(logs.keys())) + '\n')
      writelog(epoch, logs, outf)
  else:
    with open(lfname,'a') as outf:
      writelog(epoch, logs, outf)
  # save generator model
  every = 500
  if epoch <= 1000:
    every = 100
  if epoch % every == 0:
    generator.save(genfname + str(epoch))
  return

### build Dataset from data file
def get_real_data(filename, batchsize):
  indatas = []
  targets = []
  with open(filename, 'r') as handle:
    reader = csv.reader(handle)
    # parse header
    target_cols = []
    indata_cols = []
    headrow = next(reader)
    for idx in range(len(headrow)):
      label = headrow[idx]
      if label == 'id':
        continue
      if label.find('target') == 0:
        target_cols.append(idx)
      else:
        indata_cols.append(idx)
    # parse data
    for row in reader:
      trgts = [ float(row[i]) for i in target_cols ]
      datas = [ [float(row[i])] for i in indata_cols]
      targets.append(trgts)
      indatas.append(datas)
  # package in tensorflow Dataset
  data = Dataset.from_tensor_slices((indatas, targets))
  data = data.shuffle(len(indatas)).batch(batchsize)
  return data


### main -----------------------------------------------------------------------
if __name__ == '__main__':
  parser = argparse.ArgumentParser(
                description='microbiome generative adversarial network',
                formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  parser.add_argument('--version', action='version', version=__version__)
  parser.add_argument('--verbose', type=distutils.util.strtobool,
                      dest='verbose', help='runtime informatoin?',
                      metavar='y|n')

  # data source
  parser.add_argument('-f', '--file', dest='file', help='data file',
                      metavar='DATA.csv', required=True)
  parser.add_argument('--outdir', dest='outdir',
                      help='output directory', metavar='OUT')
  parser.add_argument('--data_dim', dest='data_dim', type=int,
                      help='size of data vector', metavar='N')
  parser.add_argument('--label_dim', dest='label_dim', type=int,
                      help='number of labels', metavar='N')

  # data pre-processing
  parser.add_argument('--batch_size', dest='batch_size', type=int,
                      help='training batch size', metavar='N')

  # neural network architecture
  parser.add_argument('--gen_blocks', dest='gen_blocks', type=int,
                      help='number of generator blocks', metavar='N')
  parser.add_argument('--disc_blocks', dest='disc_blocks', type=int,
                      help='number of discriminator blocks', metavar='N')
  parser.add_argument('--filters', dest='n_filters', type=int,
                      help='number of neurons or filters', metavar='N')
  parser.add_argument('--lipschitz', dest='lipschitz', choices=['gp', 'sn'],
                      help='lipschitz constraint to use')

  # training regime
  parser.add_argument('--epochs', dest='epochs', type=int,
                      help='number of training epochs', metavar='N')
  parser.add_argument('--optimizer', dest='optimizer',
                      choices=['adam', 'rmsprop'],
                      help='optimizer to use')
  parser.add_argument('--learning_rate', dest='learn_rate', type=float,
                      help='base learning rate for optimizer', metavar='N')
  parser.add_argument('--learning_rate_mult', dest='learn_rate_mult',
                      type=float,
                      help='discriminator learning rate multiplier',
                      metavar='N')
  parser.add_argument('--update_n', dest='update_n', type=int,
                      help='update discriminator N-times per generator',
                      metavar='N')

  parser.set_defaults(verbose=True,
                      file=None,
                      outdir=None,
                      data_dim=256,
                      label_dim=1,
                      batch_size=64,
                      gen_blocks=8,
                      disc_blocks=4,
                      n_filters=128,
                      lipschitz='sn',
                      optimizer='adam',
                      learn_rate=1.0e-5,
                      learn_rate_mult=2.0,
                      update_n=2,
                      epochs=1000)

  args = parser.parse_args()

  # set tensorflow log level
  if not args.verbose:
    os.environ['TF_CPP_MIN_LOG_LEVEL']='1'


  """
  # get 'real' labelled data
  train_data = get_real_data(args.file, args.batch_size)
  if args.verbose:
    print(train_data.element_spec)

  ## XX

  if args.verbose:
    gan.summary()

  # set up output base name
  outbasename = args.file
  if not args.outdir:
    args.outdir = './'
  basename = os.path.basename(args.file)
  if not os.path.exists(args.outdir):
    sys.stderr.write('ERRR: outdir {} in existential crisis'\
                     .format(args.outdir))
    sys.exit(1)
  outbasename = os.path.join(args.outdir, basename)

  # set up checkpoints
  #ckptfname = outbasename + '.ckpt'
  #ckpt = callbacks.ModelCheckpoint(ckptfname, monitor='g_loss')

  # set up tensorboard
  #tbdir = outbasename + '.tblog'
  #tb = callbacks.TensorBoard(log_dir=tbdir)

  # set up generator logging
  genfname  = outbasename + '.gen.model'
  cbk = callbacks.LambdaCallback(on_epoch_end=lambda epoch,logs: \
                                              store_model(epoch, logs,
                                                          gan.generator,
                                                          genfname))

  # fit generative adverserial network
  gan.fit(train_data, epochs=args.epochs, callbacks=[cbk], verbose=args.verbose)

  # save final generator model
  gan.generator.save(genfname)
  """
